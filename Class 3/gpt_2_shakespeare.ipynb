{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "1964cb45-7104-4e5a-b347-ae3e87162b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=f463ce640b528c8c6872de79b091c3621da297c0c01bd515532cbe4de7816bec\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "83f953c8-3e56-4411-8904-2f0bfb819bbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 397Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.43Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 710Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:26, 19.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 577Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.70Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.66Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "d976c31c-2412-494a-f323-ed0096c1b14d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "a4bda47e-f273-4a4a-eba8-827ba1a8cc94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "A: There is â€“ there is no more than two.\n",
            "\n",
            "Q: You have said, \"if we are to have a third world, we need to extend our civilisation to two planets? Or if we are to have a third\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "57e74cb5-8175-4983-c8e6-b0ec2848da40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 14:04:28--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.166.200, 52.216.241.6, 52.216.97.189, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.166.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.1â€™\n",
            "\n",
            "nietzsche.txt.1     100%[===================>] 586.82K  1.90MB/s    in 0.3s    \n",
            "\n",
            "2023-03-22 14:04:28 (1.90 MB/s) - â€˜nietzsche.txt.1â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "d12e0ccc-f926-4d09-e788-7934dfca7613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 177MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "6e412413-5163-4940-8c03-26e6d60a2c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 13:24:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-03-22 13:24:29 (32.4 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "07875a7e-39cf-4a6c-e12d-937d31f550b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.67] loss=3.40 avg=3.40\n",
            "[2 | 9.87] loss=3.25 avg=3.32\n",
            "[3 | 12.09] loss=3.33 avg=3.32\n",
            "[4 | 14.32] loss=3.27 avg=3.31\n",
            "[5 | 16.55] loss=3.22 avg=3.29\n",
            "[6 | 18.79] loss=3.18 avg=3.27\n",
            "[7 | 21.04] loss=3.20 avg=3.26\n",
            "[8 | 23.29] loss=3.33 avg=3.27\n",
            "[9 | 25.55] loss=3.13 avg=3.26\n",
            "[10 | 27.83] loss=3.23 avg=3.25\n",
            "[11 | 30.11] loss=3.20 avg=3.25\n",
            "[12 | 32.39] loss=3.07 avg=3.23\n",
            "[13 | 34.67] loss=3.16 avg=3.23\n",
            "[14 | 36.96] loss=3.26 avg=3.23\n",
            "[15 | 39.25] loss=3.19 avg=3.23\n",
            "[16 | 41.53] loss=3.07 avg=3.22\n",
            "[17 | 43.81] loss=2.96 avg=3.20\n",
            "[18 | 46.10] loss=3.18 avg=3.20\n",
            "[19 | 48.37] loss=3.23 avg=3.20\n",
            "[20 | 50.62] loss=3.21 avg=3.20\n",
            "[21 | 52.87] loss=3.06 avg=3.19\n",
            "[22 | 55.12] loss=3.14 avg=3.19\n",
            "[23 | 57.38] loss=3.14 avg=3.19\n",
            "[24 | 59.62] loss=3.15 avg=3.19\n",
            "[25 | 61.86] loss=3.06 avg=3.18\n",
            "[26 | 64.11] loss=3.17 avg=3.18\n",
            "[27 | 66.35] loss=2.97 avg=3.17\n",
            "[28 | 68.57] loss=3.05 avg=3.17\n",
            "[29 | 70.81] loss=3.11 avg=3.16\n",
            "[30 | 73.04] loss=3.15 avg=3.16\n",
            "[31 | 75.27] loss=3.09 avg=3.16\n",
            "[32 | 77.50] loss=3.04 avg=3.16\n",
            "[33 | 79.74] loss=3.05 avg=3.15\n",
            "[34 | 81.96] loss=3.01 avg=3.15\n",
            "[35 | 84.19] loss=3.13 avg=3.15\n",
            "[36 | 86.42] loss=3.03 avg=3.14\n",
            "[37 | 88.65] loss=3.20 avg=3.15\n",
            "[38 | 90.89] loss=2.99 avg=3.14\n",
            "[39 | 93.12] loss=2.92 avg=3.13\n",
            "[40 | 95.35] loss=2.90 avg=3.13\n",
            "[41 | 97.59] loss=2.87 avg=3.12\n",
            "[42 | 99.84] loss=3.07 avg=3.12\n",
            "[43 | 102.09] loss=2.97 avg=3.11\n",
            "[44 | 104.33] loss=3.00 avg=3.11\n",
            "[45 | 106.57] loss=2.98 avg=3.11\n",
            "[46 | 108.82] loss=2.94 avg=3.10\n",
            "[47 | 111.07] loss=2.86 avg=3.10\n",
            "[48 | 113.32] loss=2.98 avg=3.09\n",
            "[49 | 115.57] loss=3.06 avg=3.09\n",
            "[50 | 117.82] loss=2.87 avg=3.09\n",
            "[51 | 120.06] loss=2.94 avg=3.08\n",
            "[52 | 122.31] loss=2.87 avg=3.08\n",
            "[53 | 124.57] loss=2.92 avg=3.07\n",
            "[54 | 126.83] loss=2.91 avg=3.07\n",
            "[55 | 129.07] loss=2.90 avg=3.07\n",
            "[56 | 131.32] loss=2.85 avg=3.06\n",
            "[57 | 133.57] loss=2.94 avg=3.06\n",
            "[58 | 135.82] loss=2.92 avg=3.05\n",
            "[59 | 138.06] loss=2.81 avg=3.05\n",
            "[60 | 140.31] loss=3.03 avg=3.05\n",
            "[61 | 142.56] loss=2.94 avg=3.05\n",
            "[62 | 144.81] loss=3.02 avg=3.05\n",
            "[63 | 147.05] loss=2.83 avg=3.04\n",
            "[64 | 149.31] loss=2.98 avg=3.04\n",
            "[65 | 151.55] loss=2.95 avg=3.04\n",
            "[66 | 153.80] loss=2.90 avg=3.04\n",
            "[67 | 156.04] loss=2.98 avg=3.03\n",
            "[68 | 158.29] loss=2.93 avg=3.03\n",
            "[69 | 160.54] loss=2.98 avg=3.03\n",
            "[70 | 162.79] loss=2.96 avg=3.03\n",
            "[71 | 165.03] loss=2.85 avg=3.03\n",
            "[72 | 167.27] loss=2.81 avg=3.02\n",
            "[73 | 169.51] loss=2.73 avg=3.02\n",
            "[74 | 171.75] loss=2.97 avg=3.02\n",
            "[75 | 174.00] loss=2.86 avg=3.01\n",
            "[76 | 176.24] loss=2.85 avg=3.01\n",
            "[77 | 178.49] loss=2.88 avg=3.01\n",
            "[78 | 180.73] loss=2.83 avg=3.00\n",
            "[79 | 182.97] loss=2.89 avg=3.00\n",
            "[80 | 185.22] loss=2.83 avg=3.00\n",
            "[81 | 187.47] loss=2.98 avg=3.00\n",
            "[82 | 189.71] loss=2.95 avg=3.00\n",
            "[83 | 191.96] loss=2.82 avg=2.99\n",
            "[84 | 194.21] loss=2.78 avg=2.99\n",
            "[85 | 196.46] loss=3.02 avg=2.99\n",
            "[86 | 198.70] loss=2.98 avg=2.99\n",
            "[87 | 200.95] loss=2.80 avg=2.99\n",
            "[88 | 203.20] loss=2.71 avg=2.98\n",
            "[89 | 205.44] loss=2.79 avg=2.98\n",
            "[90 | 207.69] loss=2.84 avg=2.98\n",
            "[91 | 209.95] loss=2.71 avg=2.97\n",
            "[92 | 212.20] loss=2.78 avg=2.97\n",
            "[93 | 214.44] loss=2.95 avg=2.97\n",
            "[94 | 216.68] loss=2.91 avg=2.97\n",
            "[95 | 218.93] loss=2.94 avg=2.97\n",
            "[96 | 221.18] loss=2.77 avg=2.96\n",
            "[97 | 223.43] loss=2.77 avg=2.96\n",
            "[98 | 225.67] loss=3.01 avg=2.96\n",
            "[99 | 227.91] loss=2.63 avg=2.96\n",
            "[100 | 230.16] loss=2.86 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " were a lot younger and wiser. \n",
            "\"And I knew that if I'd ever met Jon Snow's father, and lived with him now.\" \n",
            "\"He murdered his own brother,\" Ned reminded him, his voice deep with fear. \"And we are a long way from him now.\" \n",
            "\"What's that?\" Robb's voice was quiet and urgent. \"Have no fear, my brother Robb.\" \n",
            "\"Don't,\" Ned repeated, \"but I knew the words. Jon Snow had murdered his own brother.\" \n",
            "\"He did. You should . . . I saw his body . . . the blade. That made him alive. My own brother's body. \n",
            "You've been good, Bran. You've told me everything. All I can tell you is you loved Jon Snow, now you think he was murdered.\" \n",
            "Page 134\n",
            "\n",
            "\"I loved him. The sword had been gone for a long time, I must have remembered. I can't think of that now.\" \n",
            "\"Father knew it, Bran. I loved him, I think he loved Jon even more than I do, you told him to keep this one away, \n",
            "he's gone forever. If I let him die, he could be dead all over again.\" \n",
            "\"My hand was with your brother, I swear I told you, \n",
            "before they murdered my husband or my wife,\" Ned said. \"I mean it. Jon Snow's a brother now, and I \n",
            "knew of it. \n",
            "A voice interrupted. It came from beyond the Wall, its mouth a rasping \n",
            "pale husk. A smile crept on his face. \"Jon Snow,\" it said. \"Jon Snow.\" \n",
            "\"I know,\" he said. \"How could we trust him?\"\n",
            "\"I thought we'd found him.\" \n",
            "\"The Hand.\" \n",
            "\"How?\" Ned did not answer. \n",
            "\"He had a blade in his hand. One that was a sharp weapon. He had a shield in his hands.\" \n",
            "There was laughter. \"Jon Snow, are you hurt?\" \n",
            "Page 135\n",
            "\n",
            "\"Did we?\" \n",
            "\"What?\" \n",
            "\"My brother.\" \n",
            "\"He was not harmed. The guards caught him in his throat.\" \n",
            "He looked up at Ned. \"Father must have heard the sound. The Hound was on his knees.\" \n",
            "\"I heard him,\" Ned said in wonder. \"A man is only too quick to leap from a dead man's throat. What is it? How long \n",
            "it will take before the beast's death does not concern me. There is no truth to it. The Hand is still alive. \n",
            "The Hand is still alive, my brother. I have the strength to fight him, the man's life is at stake, no doubt.\" \n",
            "\"I know enough, Ned. My brother was a good lad. If I had believed him, he'd been so angry. That makes \n",
            "the Hound dead. He could be the last thing on my hand.\" \n",
            "\"Did I not tell you the truth?\" \n",
            "\"No, Ned. If I had, my brother might have killed himself.\" \n",
            "\"Do you trust him?\" \n",
            "\"I doubt it.\" \n",
            "\"How much?\" \n",
            "\"Four hundred and fifty thousand silver,\" the Hound replied. \"The price. That's how much Jon Snow \n",
            "has paid.\" \n",
            "\"For the Stone,\" Ned said as he strode toward the door. It was \n",
            "the middle of the night, the last chance he had to find Bran. \n",
            "\"If you believe him, Robb, you can leave here and talk to the maester in the castle to make sure he's \n",
            "willing to tell you the truth.\" \n",
            "\"I can.\" \n",
            "The door creaked open, and the Hound dropped out the light, leaving Robb behind him and his \n",
            "brother. They sat on the grass in the yard, listening to what the man had to say. He might have had a cell the\n",
            "old man wanted, Ned was afraid. It seemed to him, now that that door had been closed. That seemed to \n",
            "be it, but Robb had no word. \n",
            "\"I told him to keep the cell door closed,\" Robb reminded him. \"He did. \" \n",
            "They went outside with the rest.\" \n",
            "Page 136\n",
            "\n",
            "\"How bad is his cold? We'll go on with the other side.\" \n",
            "\"He might be cold enough in here,\" Robb said. They looked around, confused but pleased. \"Robert, \n",
            "you know the others best. Jon came this morning with a woman he found at the bottom of the hill, so that he could get her \n",
            "back safely before his sword ran out of his grasp.\" \n",
            "\"How is she?\" \n",
            "\"Robert took her to the Vale of Arryn.\" \n",
            "\n",
            "\n",
            "[101 | 245.10] loss=2.73 avg=2.95\n",
            "[102 | 247.35] loss=2.77 avg=2.95\n",
            "[103 | 249.59] loss=2.84 avg=2.95\n",
            "[104 | 251.84] loss=2.79 avg=2.95\n",
            "[105 | 254.10] loss=2.82 avg=2.94\n",
            "[106 | 256.34] loss=2.94 avg=2.94\n",
            "[107 | 258.60] loss=2.80 avg=2.94\n",
            "[108 | 260.85] loss=2.87 avg=2.94\n",
            "[109 | 263.10] loss=2.83 avg=2.94\n",
            "[110 | 265.36] loss=2.89 avg=2.94\n",
            "[111 | 267.62] loss=2.73 avg=2.93\n",
            "[112 | 269.87] loss=2.85 avg=2.93\n",
            "[113 | 272.13] loss=2.74 avg=2.93\n",
            "[114 | 274.38] loss=2.76 avg=2.93\n",
            "[115 | 276.64] loss=2.76 avg=2.93\n",
            "[116 | 278.89] loss=2.63 avg=2.92\n",
            "[117 | 281.15] loss=2.89 avg=2.92\n",
            "[118 | 283.41] loss=2.60 avg=2.92\n",
            "[119 | 285.66] loss=2.69 avg=2.91\n",
            "[120 | 287.91] loss=2.73 avg=2.91\n",
            "[121 | 290.16] loss=2.55 avg=2.91\n",
            "[122 | 292.41] loss=2.71 avg=2.90\n",
            "[123 | 294.67] loss=2.68 avg=2.90\n",
            "[124 | 296.92] loss=2.70 avg=2.90\n",
            "[125 | 299.17] loss=2.68 avg=2.89\n",
            "[126 | 301.42] loss=2.70 avg=2.89\n",
            "[127 | 303.68] loss=2.84 avg=2.89\n",
            "[128 | 305.93] loss=2.76 avg=2.89\n",
            "[129 | 308.18] loss=2.57 avg=2.88\n",
            "[130 | 310.43] loss=2.78 avg=2.88\n",
            "[131 | 312.67] loss=2.78 avg=2.88\n",
            "[132 | 314.92] loss=2.77 avg=2.88\n",
            "[133 | 317.17] loss=2.86 avg=2.88\n",
            "[134 | 319.42] loss=2.72 avg=2.88\n",
            "[135 | 321.67] loss=2.83 avg=2.88\n",
            "[136 | 323.91] loss=2.69 avg=2.87\n",
            "[137 | 326.16] loss=2.72 avg=2.87\n",
            "[138 | 328.41] loss=2.88 avg=2.87\n",
            "[139 | 330.66] loss=2.71 avg=2.87\n",
            "[140 | 332.91] loss=2.81 avg=2.87\n",
            "[141 | 335.15] loss=2.84 avg=2.87\n",
            "[142 | 337.40] loss=2.73 avg=2.87\n",
            "[143 | 339.65] loss=2.71 avg=2.86\n",
            "[144 | 341.90] loss=2.78 avg=2.86\n",
            "[145 | 344.14] loss=2.54 avg=2.86\n",
            "[146 | 346.39] loss=2.72 avg=2.86\n",
            "[147 | 348.63] loss=2.63 avg=2.85\n",
            "[148 | 350.88] loss=2.74 avg=2.85\n",
            "[149 | 353.13] loss=2.75 avg=2.85\n",
            "[150 | 355.39] loss=2.81 avg=2.85\n",
            "[151 | 357.64] loss=2.75 avg=2.85\n",
            "[152 | 359.89] loss=2.67 avg=2.85\n",
            "[153 | 362.14] loss=2.70 avg=2.85\n",
            "[154 | 364.39] loss=2.57 avg=2.84\n",
            "[155 | 366.63] loss=2.52 avg=2.84\n",
            "[156 | 368.88] loss=2.46 avg=2.83\n",
            "[157 | 371.13] loss=2.62 avg=2.83\n",
            "[158 | 373.37] loss=2.67 avg=2.83\n",
            "[159 | 375.61] loss=2.55 avg=2.83\n",
            "[160 | 377.86] loss=2.71 avg=2.82\n",
            "[161 | 380.11] loss=2.69 avg=2.82\n",
            "[162 | 382.35] loss=2.63 avg=2.82\n",
            "[163 | 384.59] loss=2.44 avg=2.82\n",
            "[164 | 386.84] loss=2.65 avg=2.81\n",
            "[165 | 389.09] loss=2.58 avg=2.81\n",
            "[166 | 391.34] loss=2.53 avg=2.81\n",
            "[167 | 393.58] loss=2.44 avg=2.80\n",
            "[168 | 395.83] loss=2.42 avg=2.80\n",
            "[169 | 398.08] loss=2.61 avg=2.80\n",
            "[170 | 400.32] loss=2.50 avg=2.79\n",
            "[171 | 402.57] loss=2.71 avg=2.79\n",
            "[172 | 404.82] loss=2.28 avg=2.78\n",
            "[173 | 407.06] loss=2.42 avg=2.78\n",
            "[174 | 409.31] loss=2.67 avg=2.78\n",
            "[175 | 411.56] loss=2.82 avg=2.78\n",
            "[176 | 413.81] loss=2.73 avg=2.78\n",
            "[177 | 416.06] loss=2.67 avg=2.78\n",
            "[178 | 418.31] loss=2.74 avg=2.78\n",
            "[179 | 420.55] loss=2.42 avg=2.77\n",
            "[180 | 422.80] loss=2.46 avg=2.77\n",
            "[181 | 425.05] loss=2.55 avg=2.77\n",
            "[182 | 427.30] loss=2.70 avg=2.77\n",
            "[183 | 429.55] loss=2.59 avg=2.76\n",
            "[184 | 431.80] loss=2.53 avg=2.76\n",
            "[185 | 434.04] loss=2.61 avg=2.76\n",
            "[186 | 436.29] loss=2.57 avg=2.76\n",
            "[187 | 438.55] loss=2.61 avg=2.76\n",
            "[188 | 440.80] loss=2.57 avg=2.75\n",
            "[189 | 443.05] loss=2.56 avg=2.75\n",
            "[190 | 445.30] loss=2.68 avg=2.75\n",
            "[191 | 447.55] loss=2.74 avg=2.75\n",
            "[192 | 449.80] loss=2.73 avg=2.75\n",
            "[193 | 452.05] loss=2.54 avg=2.75\n",
            "[194 | 454.30] loss=2.64 avg=2.75\n",
            "[195 | 456.55] loss=2.60 avg=2.74\n",
            "[196 | 458.80] loss=2.50 avg=2.74\n",
            "[197 | 461.05] loss=2.37 avg=2.74\n",
            "[198 | 463.32] loss=2.77 avg=2.74\n",
            "[199 | 465.57] loss=2.46 avg=2.73\n",
            "[200 | 467.81] loss=2.59 avg=2.73\n",
            "======== SAMPLE 1 ========\n",
            "ched as he took the red cloak off his shoulders; that was a pity.\" \n",
            "\"There was a woman in my room,\" Ren recalled. \n",
            "Krogan grunted and began to flinch. The raven flew over his head and began a dance of scales. \n",
            "The bird was so black \n",
            "that when he took the step of the raven, the blade of his longsword crashed into the door at the center of the room. \n",
            "Krogan's eyes narrowed. \"Oh, kill her?\" he gasped as his blade curled around the raven's throat. \n",
            "Page 341\n",
            "\n",
            "\"You,\" Ren said, leaning forward and holding a hand over his mouth to steady himself. \"I'm not an army man.\" \n",
            "\"Yes. That's my job.\" The raven made a slow, breathy climb toward the knight. As it swung out, it \n",
            "slurched. Grenn leapt at it with a flurry of broken cuts with his teeth. He caught it with deft \n",
            "sharpened teeth. The raven circled the knight in a cage, a cage that held no eggs. \n",
            "Ren reached out and took Ren in his scabbard. \"I will have a look inside this cage,\" he said with a shudder. \n",
            "It took another three tries to scrape Ren off his feet. \"Gods save Bran,\" he muttered under his breath. \n",
            "\"If we keep him alive, we'll have no use for him.\" \n",
            "\"I will kill him and leave the others to deal with him,\" Grenn promised. \"In the end, he's a coward, no \n",
            "better than anyone I've ever known. I mean, there's no-\" \n",
            "When he pressed his heels into his hand, a great gust of wind whacked him on the shoulder. Grenn opened his eyes and opened his \n",
            "mouth. The raven was in a thick bandage on his shoulder. \"I can cut down a half, and I've had my doubts.\" \n",
            "The raven moved past him. The blade was hard as iron against the raven's flesh. Its eyes were blood red. \n",
            "Krogan had not seen it yet. He took a breath, and stared at the direwolf. \"Agh, what are you doing here?\" \n",
            "\"The castle is quiet, m'lord,\" the raven told him. \"Your father's gone, you say. No one needs to go down here.\" \n",
            "\"Gods forbid you come,\" Grenn said. \"Let's go down there, and bring the wolves with us.\" He grabbed a \n",
            "hammer to plow through the raven's fur, and with it came the riper blow. Grenn's longsword was \n",
            "underfoot. No one answered the call. \n",
            "Ren stopped and looked behind him. The castle was shrouded in shadow, dark and empty as a \n",
            "bottle of water. He could hear the rustle of horses and the scrape of metal against the ironwood floor. The king's \n",
            "cloak stood high as a statue; it held a great oak chalice overlooking the pool. The king seemed \n",
            "younger than he was, and more handsome. The kingcloak and its twin doors were high and barred; the \n",
            "sides of the rooms were visible, the walls were covered with a pale plaster of crimson granite. \n",
            "As it was, Grenn would not have been surprised to find himself staring at the king's chest. He knew \n",
            "how quickly the queen died. \n",
            "Grenn was not that common among his men. It may be that he was born an Auroch, yet he knew more about it than any \n",
            "one of his brothers. His brother, Robert, had come to be known as the Iron Throne. The heir to the throne \n",
            "was a man of the Night's Watch when he died of high treason. The queen was buried beneath the \n",
            "castle at Braavos. Her son had taken it up with his bloodriders, and the crown was crowned on him by \n",
            "the Kingslayer. Grenn had learned that the king would live to be old forty-five, but he had only \n",
            "seen the last of the Night's Watch if he chose to stay on it. He would have been thirty when he died, \n",
            "and all three of his children would be eighty or over, each of them more than a few centuries dead. \n",
            "His two younger brothers would live to be eighty and twenty, with only the king two centuries \n",
            "ago to inherit the throne. They had been small fry until a certain Lysa Arryn, a Lannister from the \n",
            "Kingmaker's Reach, gave them her name, but they were close, and as much her son as his sister Myrcella \n",
            "Page 342\n",
            "\n",
            "had taught them in the hallways of House Lannister. The young Arryn had been twenty, though, so his father was old\n",
            "\n",
            "[201 | 481.44] loss=2.36 avg=2.73\n",
            "[202 | 483.68] loss=2.48 avg=2.73\n",
            "[203 | 485.93] loss=2.63 avg=2.72\n",
            "[204 | 488.18] loss=2.61 avg=2.72\n",
            "[205 | 490.43] loss=2.84 avg=2.72\n",
            "[206 | 492.67] loss=2.48 avg=2.72\n",
            "[207 | 494.91] loss=2.70 avg=2.72\n",
            "[208 | 497.16] loss=2.35 avg=2.72\n",
            "[209 | 499.41] loss=2.63 avg=2.72\n",
            "[210 | 501.65] loss=2.43 avg=2.71\n",
            "[211 | 503.90] loss=2.50 avg=2.71\n",
            "[212 | 506.15] loss=2.52 avg=2.71\n",
            "[213 | 508.38] loss=2.44 avg=2.71\n",
            "[214 | 510.63] loss=2.73 avg=2.71\n",
            "[215 | 512.88] loss=2.53 avg=2.70\n",
            "[216 | 515.14] loss=2.47 avg=2.70\n",
            "[217 | 517.38] loss=2.69 avg=2.70\n",
            "[218 | 519.63] loss=2.67 avg=2.70\n",
            "[219 | 521.88] loss=2.34 avg=2.70\n",
            "[220 | 524.13] loss=2.45 avg=2.69\n",
            "[221 | 526.37] loss=2.51 avg=2.69\n",
            "[222 | 528.62] loss=2.38 avg=2.69\n",
            "[223 | 530.87] loss=2.51 avg=2.69\n",
            "[224 | 533.12] loss=2.69 avg=2.69\n",
            "[225 | 535.36] loss=2.71 avg=2.69\n",
            "[226 | 537.61] loss=2.39 avg=2.68\n",
            "[227 | 539.86] loss=2.59 avg=2.68\n",
            "[228 | 542.10] loss=2.48 avg=2.68\n",
            "[229 | 544.35] loss=2.41 avg=2.68\n",
            "[230 | 546.60] loss=2.44 avg=2.67\n",
            "[231 | 548.85] loss=2.52 avg=2.67\n",
            "[232 | 551.10] loss=2.62 avg=2.67\n",
            "[233 | 553.34] loss=2.72 avg=2.67\n",
            "[234 | 555.59] loss=2.58 avg=2.67\n",
            "[235 | 557.84] loss=2.68 avg=2.67\n",
            "[236 | 560.09] loss=2.55 avg=2.67\n",
            "[237 | 562.34] loss=2.44 avg=2.67\n",
            "[238 | 564.58] loss=2.41 avg=2.66\n",
            "[239 | 566.83] loss=2.43 avg=2.66\n",
            "[240 | 569.08] loss=2.47 avg=2.66\n",
            "[241 | 571.34] loss=2.50 avg=2.66\n",
            "[242 | 573.59] loss=2.51 avg=2.66\n",
            "[243 | 575.83] loss=2.25 avg=2.65\n",
            "[244 | 578.08] loss=2.29 avg=2.65\n",
            "[245 | 580.34] loss=2.47 avg=2.65\n",
            "[246 | 582.58] loss=2.45 avg=2.64\n",
            "[247 | 584.84] loss=2.54 avg=2.64\n",
            "[248 | 587.08] loss=2.35 avg=2.64\n",
            "[249 | 589.33] loss=2.41 avg=2.64\n",
            "[250 | 591.58] loss=2.56 avg=2.64\n",
            "[251 | 593.82] loss=2.17 avg=2.63\n",
            "[252 | 596.07] loss=2.26 avg=2.63\n",
            "[253 | 598.33] loss=2.48 avg=2.63\n",
            "[254 | 600.57] loss=2.50 avg=2.62\n",
            "[255 | 602.82] loss=2.57 avg=2.62\n",
            "[256 | 605.07] loss=2.32 avg=2.62\n",
            "[257 | 607.32] loss=2.36 avg=2.62\n",
            "[258 | 609.58] loss=2.43 avg=2.62\n",
            "[259 | 611.83] loss=2.35 avg=2.61\n",
            "[260 | 614.08] loss=2.36 avg=2.61\n",
            "[261 | 616.33] loss=2.50 avg=2.61\n",
            "[262 | 618.57] loss=2.25 avg=2.61\n",
            "[263 | 620.82] loss=2.46 avg=2.60\n",
            "[264 | 623.08] loss=2.41 avg=2.60\n",
            "[265 | 625.33] loss=2.47 avg=2.60\n",
            "[266 | 627.58] loss=2.36 avg=2.60\n",
            "[267 | 629.83] loss=1.97 avg=2.59\n",
            "[268 | 632.08] loss=2.62 avg=2.59\n",
            "[269 | 634.33] loss=2.39 avg=2.59\n",
            "[270 | 636.57] loss=2.34 avg=2.59\n",
            "[271 | 638.82] loss=2.36 avg=2.58\n",
            "[272 | 641.07] loss=2.50 avg=2.58\n",
            "[273 | 643.32] loss=2.37 avg=2.58\n",
            "[274 | 645.58] loss=2.38 avg=2.58\n",
            "[275 | 647.82] loss=2.33 avg=2.58\n",
            "[276 | 650.07] loss=2.67 avg=2.58\n",
            "[277 | 652.32] loss=2.44 avg=2.58\n",
            "[278 | 654.57] loss=2.32 avg=2.57\n",
            "[279 | 656.82] loss=2.38 avg=2.57\n",
            "[280 | 659.07] loss=2.42 avg=2.57\n",
            "[281 | 661.31] loss=2.48 avg=2.57\n",
            "[282 | 663.56] loss=2.30 avg=2.57\n",
            "[283 | 665.80] loss=2.34 avg=2.56\n",
            "[284 | 668.05] loss=2.33 avg=2.56\n",
            "[285 | 670.30] loss=2.09 avg=2.56\n",
            "[286 | 672.54] loss=2.38 avg=2.55\n",
            "[287 | 674.79] loss=2.16 avg=2.55\n",
            "[288 | 677.04] loss=2.56 avg=2.55\n",
            "[289 | 679.29] loss=2.29 avg=2.55\n",
            "[290 | 681.54] loss=2.36 avg=2.54\n",
            "[291 | 683.78] loss=2.29 avg=2.54\n",
            "[292 | 686.03] loss=2.11 avg=2.54\n",
            "[293 | 688.27] loss=2.51 avg=2.54\n",
            "[294 | 690.51] loss=2.15 avg=2.53\n",
            "[295 | 692.75] loss=2.26 avg=2.53\n",
            "[296 | 695.00] loss=2.22 avg=2.53\n",
            "[297 | 697.25] loss=2.37 avg=2.53\n",
            "[298 | 699.50] loss=2.41 avg=2.52\n",
            "[299 | 701.75] loss=2.27 avg=2.52\n",
            "[300 | 703.99] loss=2.21 avg=2.52\n",
            "======== SAMPLE 1 ========\n",
            " hill at once. \n",
            "\"It smells of sulfur,\" Rickon told her. He looked down at his skin, wrinkled and red. \"It smells of \n",
            "sour.\" \n",
            "Her uncle pulled his hand away and sniffed at the odor lightly. \"M'lord,\" he said, \"you're a sour bird.\" \n",
            "\"Sweet, sweet, not sweet enough for a man like me,\" his son answered, surprised, he told her what he meant. \n",
            "She was half-staring at him and almost panting with rage. Damn you! She was only two years old now, \n",
            "so she had no cause to fret too much. She could smell it in the air, in her breath, in the air where her father \n",
            "had gone. She would never forget the sour smell of sulfur. \n",
            "The girl laughed, and he began to teach her to read, playing in the snow until he had her beak \n",
            "as well as her mother. His sweet, sweet, beautiful boy did well, Rickon knew. He was the only boy Rickon knew \n",
            "before. \n",
            "When Winter had first come, he said that he loved all his little boys, the ones \n",
            "who had been born with a black, green forehead, green cheeks, and eyes yellow from their eyesight; those were \n",
            "all sweet boys, and all those are still. \n",
            "Rickon would always remember his sweet boy, who had been seven when he was born, and who would live \n",
            "every waking man's days, but he would not remember him being proud of who he was \n",
            "yet; he would like to think that was because he was sick of being loved . . . but he never truly was \n",
            "small or small. He had small hands and some large toes, and he was a good small man, but \n",
            "the smallest \n",
            "Rickon could muster . . . and even then he had an immense sense of shame . . . a huge, empty \n",
            "blind spot in his soul that he must have known. \n",
            "\"How long have you lived?\" \n",
            "Page 492\n",
            "\n",
            "His mother glanced at him curiously. \"Seven,\" she said with a bitter smile. \"Seven years. One \n",
            "of my brothers. How many?\" \n",
            "He did not know, she could hear it too, she did not understand. \"You said so,\" \n",
            "she started, with the sweetest, most delicate smile she had ever seen, a beautiful one, but it never \n",
            "explained. \n",
            "\"All the \n",
            "father loved,\" Rickon said. \"He said so too.\" And it was true, but it could not seem to change \n",
            "his mind . . . no matter how many times he tried. He looked at the snow and heard the sound of \n",
            "his footsteps, then he looked at the snow, and he must have heard Robert's voice whispering in his ear, \n",
            "then he looked at the snow again and everything was black. He woke that day the next day feeling \n",
            "good as if he were on a warm spring afternoon, feeling free to go outside with his blanket and his heels \n",
            "flat on the ground. It took a while. And when he was done sleeping, he found himself \n",
            "looking up at the stars. \n",
            "For some reason he woke up in the wrong place, in the wrong place, in the wrong place and \n",
            "did not know why. Stupid, I'm an archer. No, no, no, \n",
            "For a long, long time he was afraid to even walk. Then he remembered a time \n",
            "when he and his brother Robert were riding together when Robert had his knight's head \n",
            "found. He went from his bed in a slow motion and down the steps to his seat, trying to push the \n",
            "beads to their limit, trying to think of anything else besides his heart. \n",
            "In the forest, he found the footprints he had made on the rock again. Jeyne had taken them and \n",
            "left, in a hurry, but Jeyne had taken a long day. She had to go back. \n",
            "\"Oh, gods,\" he whispered to himself as he closed the gap, smiling. \n",
            "The stars never made it out of the trees, so it was strange to be looking down on \n",
            "a vast, lonely, empty sky filled with stars, and you could hear the faint crunch of rocks against the \n",
            "beat of Robb's voice, and the faint sound of Robb's sword against Robb's neck as he lifted his head in \n",
            "attention, and there was nothing but the endless shadow that grew everywhere above his ear. He felt \n",
            "as though he were growing at the edge of the world. A tiny tiny, little world, a little world that did not matter \n",
            "to him at all. This whole place was a dream, the whole time he looked down and saw his \n",
            "brother\n",
            "\n",
            "[301 | 717.57] loss=2.29 avg=2.52\n",
            "[302 | 719.82] loss=2.26 avg=2.51\n",
            "[303 | 722.05] loss=2.62 avg=2.51\n",
            "[304 | 724.29] loss=2.24 avg=2.51\n",
            "[305 | 726.53] loss=2.18 avg=2.51\n",
            "[306 | 728.78] loss=2.16 avg=2.50\n",
            "[307 | 731.02] loss=2.25 avg=2.50\n",
            "[308 | 733.27] loss=2.22 avg=2.50\n",
            "[309 | 735.52] loss=2.69 avg=2.50\n",
            "[310 | 737.77] loss=2.09 avg=2.50\n",
            "[311 | 740.01] loss=2.23 avg=2.49\n",
            "[312 | 742.26] loss=2.39 avg=2.49\n",
            "[313 | 744.51] loss=2.18 avg=2.49\n",
            "[314 | 746.76] loss=2.54 avg=2.49\n",
            "[315 | 749.00] loss=2.10 avg=2.49\n",
            "[316 | 751.25] loss=2.26 avg=2.48\n",
            "[317 | 753.50] loss=2.44 avg=2.48\n",
            "[318 | 755.75] loss=2.22 avg=2.48\n",
            "[319 | 757.99] loss=2.25 avg=2.48\n",
            "[320 | 760.24] loss=2.43 avg=2.48\n",
            "[321 | 762.49] loss=2.17 avg=2.47\n",
            "[322 | 764.74] loss=2.43 avg=2.47\n",
            "[323 | 767.00] loss=2.13 avg=2.47\n",
            "[324 | 769.25] loss=2.29 avg=2.47\n",
            "[325 | 771.50] loss=2.19 avg=2.47\n",
            "[326 | 773.75] loss=2.33 avg=2.46\n",
            "[327 | 776.00] loss=2.22 avg=2.46\n",
            "[328 | 778.25] loss=2.11 avg=2.46\n",
            "[329 | 780.50] loss=2.39 avg=2.46\n",
            "[330 | 782.75] loss=2.18 avg=2.45\n",
            "[331 | 785.00] loss=2.02 avg=2.45\n",
            "[332 | 787.24] loss=2.18 avg=2.45\n",
            "[333 | 789.49] loss=2.15 avg=2.44\n",
            "[334 | 791.75] loss=2.06 avg=2.44\n",
            "[335 | 794.00] loss=2.06 avg=2.44\n",
            "[336 | 796.24] loss=2.08 avg=2.43\n",
            "[337 | 798.49] loss=2.59 avg=2.43\n",
            "[338 | 800.73] loss=2.26 avg=2.43\n",
            "[339 | 802.98] loss=1.84 avg=2.43\n",
            "[340 | 805.22] loss=2.21 avg=2.42\n",
            "[341 | 807.47] loss=2.46 avg=2.42\n",
            "[342 | 809.71] loss=2.16 avg=2.42\n",
            "[343 | 811.96] loss=2.23 avg=2.42\n",
            "[344 | 814.21] loss=2.04 avg=2.42\n",
            "[345 | 816.46] loss=2.34 avg=2.41\n",
            "[346 | 818.70] loss=2.13 avg=2.41\n",
            "[347 | 820.95] loss=2.19 avg=2.41\n",
            "[348 | 823.19] loss=2.34 avg=2.41\n",
            "[349 | 825.44] loss=2.33 avg=2.41\n",
            "[350 | 827.70] loss=2.15 avg=2.41\n",
            "[351 | 829.95] loss=2.21 avg=2.40\n",
            "[352 | 832.20] loss=2.14 avg=2.40\n",
            "[353 | 834.44] loss=2.12 avg=2.40\n",
            "[354 | 836.69] loss=1.99 avg=2.39\n",
            "[355 | 838.95] loss=2.14 avg=2.39\n",
            "[356 | 841.19] loss=2.30 avg=2.39\n",
            "[357 | 843.45] loss=2.23 avg=2.39\n",
            "[358 | 845.70] loss=2.29 avg=2.39\n",
            "[359 | 847.95] loss=2.17 avg=2.38\n",
            "[360 | 850.20] loss=1.78 avg=2.38\n",
            "[361 | 852.45] loss=1.99 avg=2.37\n",
            "[362 | 854.69] loss=1.90 avg=2.37\n",
            "[363 | 856.94] loss=1.95 avg=2.37\n",
            "[364 | 859.19] loss=2.00 avg=2.36\n",
            "[365 | 861.44] loss=2.47 avg=2.36\n",
            "[366 | 863.69] loss=1.95 avg=2.36\n",
            "[367 | 865.93] loss=2.14 avg=2.36\n",
            "[368 | 868.18] loss=2.25 avg=2.36\n",
            "[369 | 870.43] loss=2.20 avg=2.35\n",
            "[370 | 872.68] loss=2.38 avg=2.35\n",
            "[371 | 874.92] loss=2.25 avg=2.35\n",
            "[372 | 877.17] loss=1.88 avg=2.35\n",
            "[373 | 879.42] loss=2.17 avg=2.35\n",
            "[374 | 881.67] loss=2.08 avg=2.34\n",
            "[375 | 883.91] loss=2.17 avg=2.34\n",
            "[376 | 886.15] loss=2.18 avg=2.34\n",
            "[377 | 888.40] loss=2.34 avg=2.34\n",
            "[378 | 890.65] loss=2.14 avg=2.34\n",
            "[379 | 892.93] loss=2.05 avg=2.33\n",
            "[380 | 895.20] loss=2.05 avg=2.33\n",
            "[381 | 897.48] loss=2.38 avg=2.33\n",
            "[382 | 899.79] loss=2.27 avg=2.33\n",
            "[383 | 902.08] loss=2.13 avg=2.33\n",
            "[384 | 904.36] loss=1.88 avg=2.33\n",
            "[385 | 906.62] loss=2.11 avg=2.32\n",
            "[386 | 908.86] loss=2.17 avg=2.32\n",
            "[387 | 911.11] loss=2.06 avg=2.32\n",
            "[388 | 913.35] loss=2.08 avg=2.32\n",
            "[389 | 915.60] loss=1.94 avg=2.31\n",
            "[390 | 917.84] loss=1.96 avg=2.31\n",
            "[391 | 920.08] loss=1.88 avg=2.30\n",
            "[392 | 922.32] loss=2.16 avg=2.30\n",
            "[393 | 924.58] loss=2.05 avg=2.30\n",
            "[394 | 926.82] loss=1.97 avg=2.30\n",
            "[395 | 929.07] loss=2.10 avg=2.30\n",
            "[396 | 931.32] loss=2.07 avg=2.29\n",
            "[397 | 933.56] loss=1.82 avg=2.29\n",
            "[398 | 935.81] loss=2.48 avg=2.29\n",
            "[399 | 938.06] loss=2.24 avg=2.29\n",
            "[400 | 940.30] loss=2.09 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            " shoulder. \n",
            "Page 394\n",
            "\n",
            "\"What is wrong so long as you keep the king's purse?\" \n",
            "Robb glanced around the castle then, and Cersei Lannister, shocked and angry at the mention of her son inside \n",
            "the door, looked around anxiously and thought, Why is everyone shouting at me? Cersei was a royal princess. She \n",
            "would never \n",
            "have said that to Robb. There was not shame in it. Even the high lords were angered. \n",
            "\"Why does Lord Tywin wear chainmail and fasten it with \n",
            "spiked metal around his neck?\" The king's lady demanded. How was she to know that? He had won her, \n",
            "she supposed, had she not? Lord Eddard was not the king's brother. \"No one can wield the \n",
            "knight's sigil if he steals it from his belt,\" her son pleaded. \"The Seven forget. Lord Eddard \n",
            "of the Eyrie is not king.\" And if he had, then that would mean the end of the realm all around, \n",
            "yet what would he do? He did not even know what was wrong with him. \n",
            "\"The Seven are gone,\" Robb said. \n",
            "\"I want to tell them, Princess.\" \n",
            "Robb's face fell. \"It is not just the gods they ask of them. They ask it. Gods and men, they ask \n",
            "Us.\" \n",
            "The king's younger brother stood tall. \"I told you to do what I took you to do. Do something. Tell them.\" \n",
            "\"Why would gods be so cruel to men?\" Robb insisted. \"Why would we want swords in our own \n",
            "hands.\" \n",
            "Robb shook his head. \n",
            "Now there was no denying. \"They are cruel,\" Cersei Lannister said, \"the Starks have practiced their \n",
            "common good, they have a long and noble fighting tradition. We do not claim them for anything, not for the \n",
            "knight or the sword. I don't believe in gods.\" \n",
            "\"I believe in men,\" Robb promised. \n",
            "And they don't believe in us. Cersei Lannister was the first to protest. \"That is nothing to do with you. \n",
            "You have done nothing wrong with your life,\" she told him. She had no doubt that Brynden would come after \n",
            "him, in his stead, and he had not even dared to attack her. She had said it openly. \"If you would go with \n",
            "them, I would take you back to the Eyrie and see if the knights went forth to attack you . . . \n",
            "I believe you with all my heart.\" \n",
            "\"You do?\" \n",
            "The queen's voice rose. \"I do not believe in gods. I do not believe in men. I believe in a living body.\" \n",
            "It was only a lie. She had never shamed him, never threatened him, but she had told it all. \"I believe in \n",
            "the living here at Winterfell.\" \n",
            "\"Yes?\" \n",
            "\"And I believe in the dead here at Riverrun, in the holdfast and the castle and my ward.\" She took his hand, then he \n",
            "was gone. There was nothing to be seen of him. \"And I believe in the missing and the damned.\" \n",
            "\"And the damned?\" \n",
            "\"A brother of mine who does not live is a traitor to my life,\" Ned said. \"We will hear no other truth from the \n",
            "King's Tower. The Wall rises from the sea, my lord.\" \n",
            "Cersei stood in the door of the castle, frowning. \"Did you know?\" she asked. She had no time to protest. \n",
            "\"I did.\" \n",
            "\"Can you imagine that, Mother?\" the queen said thoughtfully. She had never had much of a sense, but she was \n",
            "worried. She took another sip of wine and considered the matter delicately. \n",
            "\"I will not have you troubled,\" she told her children. \"The thought \n",
            "would bring me to Bran. I thought it best to go on as if it was nothing more than a dream.\" \n",
            "Robert's eyes twinkled. \"What would Bran be? To die for him?\" \n",
            "Cersei frowned. \"I would not have . . . not for the sake of his life, but for the sake of the realm.\" It was true. \n",
            "\"The realm?\" Ned asked. The king's frown was a flicker. He did not speak, it turned out. \n",
            "Cersei frowned. \"This is not what it seemed.\" In her madness, she had thought it a great kindness, a wordless \n",
            "suggestion that left no doubt as to the truth of her husband's true intentions. \"You do not see him at his \n",
            "inn\n",
            "\n",
            "[401 | 953.68] loss=2.24 avg=2.29\n",
            "[402 | 955.92] loss=2.02 avg=2.28\n",
            "[403 | 958.16] loss=1.82 avg=2.28\n",
            "[404 | 960.40] loss=1.88 avg=2.28\n",
            "[405 | 962.65] loss=2.01 avg=2.27\n",
            "[406 | 964.90] loss=1.69 avg=2.27\n",
            "[407 | 967.14] loss=2.62 avg=2.27\n",
            "[408 | 969.38] loss=1.92 avg=2.27\n",
            "[409 | 971.67] loss=2.17 avg=2.27\n",
            "[410 | 973.95] loss=1.88 avg=2.26\n",
            "[411 | 976.19] loss=2.08 avg=2.26\n",
            "[412 | 978.43] loss=1.65 avg=2.25\n",
            "[413 | 980.67] loss=1.96 avg=2.25\n",
            "[414 | 982.91] loss=1.83 avg=2.25\n",
            "[415 | 985.16] loss=1.94 avg=2.24\n",
            "[416 | 987.41] loss=1.72 avg=2.24\n",
            "[417 | 989.66] loss=1.87 avg=2.23\n",
            "[418 | 991.90] loss=1.66 avg=2.23\n",
            "[419 | 994.15] loss=1.94 avg=2.23\n",
            "[420 | 996.40] loss=2.41 avg=2.23\n",
            "[421 | 998.64] loss=2.18 avg=2.23\n",
            "[422 | 1000.89] loss=1.93 avg=2.22\n",
            "[423 | 1003.13] loss=1.91 avg=2.22\n",
            "[424 | 1005.38] loss=2.07 avg=2.22\n",
            "[425 | 1007.63] loss=2.02 avg=2.22\n",
            "[426 | 1009.88] loss=1.96 avg=2.21\n",
            "[427 | 1012.12] loss=1.68 avg=2.21\n",
            "[428 | 1014.37] loss=2.12 avg=2.21\n",
            "[429 | 1016.62] loss=2.03 avg=2.21\n",
            "[430 | 1018.87] loss=1.99 avg=2.20\n",
            "[431 | 1021.12] loss=1.90 avg=2.20\n",
            "[432 | 1023.37] loss=1.71 avg=2.20\n",
            "[433 | 1025.62] loss=2.10 avg=2.20\n",
            "[434 | 1027.86] loss=1.69 avg=2.19\n",
            "[435 | 1030.11] loss=1.69 avg=2.18\n",
            "[436 | 1032.36] loss=2.11 avg=2.18\n",
            "[437 | 1034.61] loss=1.91 avg=2.18\n",
            "[438 | 1036.85] loss=1.79 avg=2.18\n",
            "[439 | 1039.10] loss=1.95 avg=2.18\n",
            "[440 | 1041.35] loss=1.92 avg=2.17\n",
            "[441 | 1043.61] loss=1.76 avg=2.17\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13461f2-5343-4549-a51d-38da2814330c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? \n",
            "He thought so. My father told me so, and he knew it. \n",
            "The Wall, he thought, and he knew it, and he knows it, and he's afraid of the Dothraki. He's afraid of the \n",
            "Shadow. \"The Others,\" he said. \"They're gone,\" he told Maester Luwin. \"They're dead, and no one knows how they \n",
            "got here or what's happened to them. They're all dead, all dead. The only thing left is for you to wake the \n",
            "nerves of the dragons. You have to wake them, gods have mercy.\" \n",
            "\"The Others?\" asked Ser Boros. \n",
            "\"They're dead,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "7f7e3b5c-e6a0-4c8d-fd21-bf11981c73af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "id": "g7K9X3K8TEwj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}